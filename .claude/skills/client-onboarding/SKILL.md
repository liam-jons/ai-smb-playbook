---
name: client-onboarding
description: This skill should be used when the user asks to "onboard a new client", "create a client config", "set up a new playbook deployment", "generate a client JSON", "add a new client to the playbook", "configure a new client", "build a config from a transcript", or "deploy the playbook for [company name]". Guides the consultant through extracting client details from training transcripts and website analysis, generating overlay content (brand voice, recurring tasks, ROI examples), and producing a validated JSON config file at app/public/clients/{slug}.json ready for deployment.
---

# Client Onboarding Skill

Automate creation of new client deployments for the AI SMB Playbook. Transform a training transcript and website URL into a validated, commit-ready `ClientConfig` JSON file through a two-phase guided conversation.

## Required Inputs

Gather these three inputs before starting extraction:

1. **Training transcript or summary** — pasted inline or as a file path (`.planning/client-specific/{id}-{name}/`)
2. **Client website URL** — for brand voice analysis via WebFetch
3. **Client slug** — lowercase, hyphens only (e.g. `acme-industries`). Confirm with the consultant.

**Recommended prerequisite:** Run `/client-research` first to generate a comprehensive research document. The research output provides richer brand voice data, team details, and industry context than a basic website scrape. The document is saved to `.planning/client-specific/{slug}/{name}-site-content.md`.

Optional: explicit overrides for any field, "no developer track" directive, additional context documents. If the consultant provides an explicit override for a field, use that value directly and skip extraction for that field.

**If no transcript is available:** Rely entirely on the research document and consultant Q&A. Flag that recurring tasks, ROI examples, and domain-specific fields will need heavier consultant input.

## Interaction Model

This skill is a consultant-guided tool, not a fully autonomous pipeline. Follow these principles:

- **Never guess low-confidence fields** — ask the consultant explicitly. The cost of asking is low; the cost of a wrong value in the deployed config is high.
- **Present extractions with source quotes** — when showing a derived value, cite where it came from (e.g. "From the transcript: 'team of fifteen' → `teamSize: medium`").
- **Ask for explicit confirmation at each Phase 2 group** — do not proceed to the next group until the consultant confirms or corrects.
- **Request permission before any git operations** — never commit or push without explicit consultant approval.
- **Use confidence tiers** — `field-mapping.md` defines three tiers. High-confidence fields: present as statements for confirmation. Medium-confidence fields: present with source quote. Low-confidence fields: ask explicitly rather than inferring.

## Phase 1: Extraction and Draft Generation

### Step 1 — Read the template

Read the template at `app/public/clients/_template.json` and the schema at `app/src/config/client-config-schema.ts` to confirm the current field structure.

### Step 2 — Analyse the transcript

Read the training transcript/summary. Extract in a single pass:

| Category | Fields to extract |
|----------|-------------------|
| Company identity | `companyName`, `companyShortName`, training date, attendees |
| Industry & domain | `industry`, `industryContext`, `complianceArea`, `certificationName` |
| Team context | `teamSize` (small/medium/large), `sensitiveDataDescription` |
| Developer signals | Whether a dev-focused session exists (`hasDeveloperTrack`), `techStack`, `testingTool`, `database`, `webApplications` |
| Recurring tasks | Concrete tasks mentioned → `exampleRecurringTasks` array (aim for 4) |
| Domain specifics | `primaryProduct`, `reportDataSource`, `clientOnboardingType`, `complianceStakeholders` |

Flag low-confidence extractions explicitly: "I found a mention of 'Cypress' as a testing tool — is that correct?"

For the full field-by-field mapping with derivation rules and defaults, consult **`references/field-mapping.md`**.

### Step 3 — Scrape the client website

If a research document already exists at `.planning/client-specific/{slug}/{name}-site-content.md` (generated by `/client-research`), use that as the primary source for brand and company context instead of scraping from scratch. Supplement with a targeted WebFetch only if specific details are missing.

Otherwise, use WebFetch (or Firecrawl MCP if available) to fetch the client's homepage and key pages (about, services, contact). Extract:

- Brand personality and tone
- Values and mission statements
- Audience description
- Preferred terminology
- Service/product descriptions

If web fetching is unavailable, note this limitation and request the consultant provide brand context manually.

### Step 4 — Derive automatic fields

Apply derivation rules to populate fields that do not require explicit input:

- `appTitle` → `"{companyShortName} AI Playbook"`
- `companyUrlDisplay` → strip protocol (`https://`), `www.` subdomain, and trailing slash from `companyUrl`
- `localStoragePrefix` → `"{slug}-playbook"`
- `emailSubjectPrefix` → `"{companyShortName} AI Playbook"`
- `welcomeSubtitle` → `"Getting started with AI at {companyShortName}"`
- `testingToolDocs` → `"the {testingTool} docs"` (if dev track enabled)
- `feedbackEmail` → `"liam@aisolutionhub.co.uk"` (consultant default)
- `feedbackSenderEmail` → `"playbook@feedback.aisolutionhub.co.uk"` (fixed)
- `primaryAiTool` → `"Claude"` (always)
- `metaDescription` → `"Practical guidance for getting the most from Claude AI"` (generic)

### Step 5 — Generate overlay content

Generate all overlay content using transcript + website analysis. For detailed guidance on each overlay type, consult **`references/overlay-generation.md`**.

**Brand voice** (`overlays.brandVoice.frameworkExamples`): Generate all seven framework steps (keys `"1"` through `"7"`) plus `headStartContent`. Each step should be a substantive paragraph, not a placeholder.

**Recurring tasks** (`overlays.recurringTasks.examples`): Generate 3-4 examples as `{ title, description }` objects. Each description should explain what the task does, what data it uses, and how Claude helps.

**ROI examples** (`overlays.roi.clientExamples`): Generate 2-3 examples keyed by descriptive task IDs. Each has `{ title, description }`. Flag that quantitative estimates are illustrative.

### Step 6 — Determine section and starter kit configuration

**Sections:** Default to `{ enabled: null, disabled: [] }` (all sections visible). Only recommend disabling sections if the training content clearly indicates they are not relevant.

**Starter kit categories:** Recommend categories based on the client profile. Available custom categories:
- `developer-tools` — if `hasDeveloperTrack: true`
- `creative-design` — if the client does design/creative work
- `business-development` — if sales/proposals/tenders discussed
- `integration-specific` — if API/webhook work discussed
- `compliance-security` — if compliance was a significant topic

### Step 7 — Assemble the draft

Assemble the complete `ClientConfig` JSON. Do not write to disk yet — hold in memory for Phase 2 review.

## Phase 2: Grouped Review

Present the draft in logical groups. After each group, ask: "Are these correct, or should I change anything?" Do not proceed to the next group until the consultant confirms.

Present company details and derived fields as a key-value list. Present brand voice steps as readable prose (not raw JSON).

### Group 1 — Company details
Present: `companyName`, `companyShortName`, `companyUrl`, `companyUrlDisplay`, `appTitle`, slug, `localStoragePrefix`, `emailSubjectPrefix`, `consultantName`, `trainingDate`, `welcomeSubtitle`. Ask: "Are these correct?"

### Group 2 — Industry and domain
Present: `industry`, `industryContext`, `teamSize`, `complianceArea`, `certificationName`, `sensitiveDataDescription`, `sensitiveDataLabel`, `complianceStakeholders`, `primaryProduct`, `primaryProductDescription`, `reportDataSource`, `clientOnboardingType`, `exampleRecurringTasks`. Ask: "Are these correct, or should I change anything?"

### Group 3 — Developer track
If `hasDeveloperTrack: true`, present: `techStack`, `testingTool`, `testingToolDocs`, `database`, `webApplications`, `domainSpecificForm`. Ask: "Are these correct?" Skip entirely if no developer track.

### Group 4 — Brand voice overlays
Present all seven framework steps as numbered, readable paragraphs. This is the longest review step. Allow editing individual steps, regenerating specific steps, or approving the block. Also present `headStartContent`. If the consultant requests regeneration of a specific step, regenerate only that step using the same source material plus any new guidance provided. Ask: "Would you like to approve, edit, or regenerate any of these?"

### Group 5 — Recurring tasks and ROI
Present recurring task examples first, then ROI examples, as separate sub-sections. Allow add/remove/edit. Ask: "Are these appropriate for the client?"

### Group 6 — Sections and starter kit
Present section configuration and starter kit categories. Ask: "Are these correct?"

## Phase 3: Write, Validate, and Deploy

### Write the file

Before writing, check if `app/public/clients/{slug}.json` already exists. If it does, ask the consultant whether to overwrite or choose a different slug.

Write the final JSON to `app/public/clients/{slug}.json`.

### Run validation

Execute the validation pipeline. For the full checklist, consult **`references/validation-checklist.md`**.

**Schema conformance:**
- All required `siteConfig` fields present and non-empty
- No `[placeholder]` values remaining (regex: `\[.*?\]`)
- `hasDeveloperTrack` consistency (if `false`, dev fields absent; if `true`, `techStack` populated)
- `overlays.brandVoice.frameworkExamples` has keys `"1"` through `"7"`
- `overlays.recurringTasks.examples` is non-empty with `{ title, description }` entries
- `exampleRecurringTasks` array has at least 1 entry (ideally 4)

**Build verification:**
```bash
cd app && bun run build
```

Report any build errors and attempt to fix them.

### Output deployment guidance

After validation passes, output:

```
Next steps:
1. Review the file: app/public/clients/{slug}.json
2. Test locally: cd app && bun run dev, then visit http://localhost:4100?client={slug}
3. When ready, commit and push to deploy
4. Add subdomain in Vercel: {slug}.playbook.aisolutionhub.co.uk
5. Verify SSL (automatic with Vercel wildcard)
6. Test live: company name, developer track, feedback email, starter kit categories
```

Offer to commit and push (with explicit consultant confirmation before any git operations).

## Valid Section Slugs

Reference for `sections.enabled` / `sections.disabled` values:

| Track | Slugs |
|-------|-------|
| Both | `welcome`, `context`, `sessions`, `skills-extensions`, `governance`, `brand-voice`, `recurring-tasks`, `roi-measurement`, `starter-kit` |
| General only | `reliable-output` |
| Developer only | `claude-md`, `documentation`, `codebase-mapping`, `hallucinations`, `regression-testing`, `mcp-usage`, `plugins`, `technical-debt` |

## Additional Resources

### Reference Files

For detailed field-by-field guidance:
- **`references/field-mapping.md`** — Complete field mapping with sources, derivation rules, and defaults for every `ClientConfig` field
- **`references/overlay-generation.md`** — Detailed generation guidance for brand voice (7 steps), recurring tasks, ROI examples, and `headStartContent`
- **`references/validation-checklist.md`** — Full schema conformance checks, content quality checks, and starter kit category validation

### Ground Truth

Evaluate output quality by comparing against the existing Phew deployment:
- **`app/public/clients/phew.json`** — Complete, production-quality client config
- **`.planning/client-specific/00-phew/`** — Source training data used to create the Phew config
