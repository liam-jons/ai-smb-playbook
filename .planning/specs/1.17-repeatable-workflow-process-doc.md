# Spec 1.17 — Repeatable Workflow Process Doc

> **Phase 2 build agent:** Agent 6 — Welcome, Process Doc & Integration Content
> **Track:** N/A (internal process document — not a playbook section, but viewable from within the app)
> **Output:** `docs/repeatable-workflow.md`

## Purpose

This is an internal process document that captures the end-to-end workflow for delivering post-training follow-up materials to clients. It serves two audiences: Liam (the consultant) as a repeatable playbook for future engagements, and as a showcase piece demonstrating the methodology to prospective clients.

The Phew! project is the worked example throughout — every step in the process is illustrated by what actually happened during this engagement. The document itself is a product of the workflow it describes (the meta-narrative).

The output is a standalone markdown file at `docs/repeatable-workflow.md`, targeting 2–3 pages of practical, concise content. It is also linked from within the playbook app (from the welcome/orientation page or navigation) so Phew! staff can see the process that produced their deliverable.

## Source References

| File | Path | What to extract |
|------|------|-----------------|
| Handoff doc | `.planning/phew-follow-up-handoff.md` | The full phased build process (Phases 0–4), agent allocation, coverage checklist, and the "Notes for Build Session" section. This is the primary structural reference for the process doc. |
| Training session 1 summary | `.planning/source-context/phew-training-ai-and-the-art-of-the-possible-summary.md` | Context on how the training sessions were delivered, topics covered, and the Granola/Otter.ai transcription approach. |
| Training session 2 summary | `.planning/source-context/phew-training-claude-code-and-qa-summary.md` | Context on the dev/QA session: atomic task decomposition, parallel agents, skills management. These concepts are both training content AND the methodology used to build the deliverable. |
| Client feedback | `.planning/source-context/feedback-from-phew.md` | The feedback that triggered this entire project. Shows how client input becomes the requirements for the deliverable. |
| App tech stack | `.planning/research/app-tech-stack.md` | Technology choices for the interactive app — referenced in Step 6 (deployment). |
| Frontend skills review | `.planning/research/frontend-skills-review.md` | Design guidelines and Build Agent Checklist (copied in full at the end of this spec). |

## Content Outline

The process doc describes a 7-step workflow. Each step should include: what happens, which tools are used, approximate time, practical tips, and a worked example from the Phew! engagement. The entire document should be written so that another consultant could follow it for a different client.

### Step 1 — Client Meeting(s): Record and Transcribe

**What happens:** Deliver training session(s) to the client. Record the session using a transcription tool that runs alongside the video call or in-person meeting.

**Tools:**
- **Granola** (macOS) — sits on top of the meeting, captures audio, produces structured notes with AI summary. Works with any video call app.
- **Otter.ai** — alternative; browser-based, good for remote sessions. Produces timestamped transcript with speaker identification.
- Video call platform (Google Meet, Zoom, Teams) — the transport layer.

**Time estimate:** The duration of the training itself (1–3 hours per session). No additional time — recording is passive.

**Tips:**
- Use Granola's real-time note-taking to capture your own observations alongside the transcript. These become useful context that the AI summary might miss.
- If delivering multiple sessions to different audiences (as with Phew! — one general, one dev-focused), keep them as separate transcripts. This preserves audience segmentation in the output.
- Confirm recording consent with the client beforehand. UK GDPR applies.

**Phew! example:** Two sessions delivered on 11 Feb 2026 — "AI and the Art of the Possible" (broad audience) and "Claude Code, QA Approaches" (dev/QA focus). Both recorded via Granola, producing structured summaries with action items.

### Step 2 — Summarise and Identify Focus Areas

**What happens:** The transcription tool produces an AI-generated summary. Review and augment this with your own notes. Identify the key topics, client pain points, and explicit requests that should shape the deliverable.

**Tools:**
- Granola's built-in AI summary (automatic)
- Manual review — read through the summary, cross-reference with your own notes, highlight gaps

**Time estimate:** 15–30 minutes per session transcript.

**Tips:**
- Pay close attention to questions the client asked — these reveal what they found confusing or wanted more depth on.
- Capture explicit requests verbatim. In Phew!'s case, the MD specifically asked for "a summary document or cheat-sheet highlighting the core techniques."
- Look for audience differences. If some attendees were more technical, note which topics resonated with which group — this informs whether to create multiple tracks or a single linear document.

**Phew! example:** The Granola summaries identified key themes: context management, skills/extensions, governance, session handling. The client feedback explicitly requested take-away materials. The dev session surfaced specific topics: CLAUDE.md files, codebase mapping, regression testing, MCP safety. These became the playbook's two-track structure (General and Developer).

### Step 3 — Provide Context to Claude and Plan

**What happens:** Load the session summaries, client feedback, and any relevant background materials into Claude as context. Ask Claude to produce a structured plan for the deliverable.

**Tools:**
- **Claude** (claude.ai, Claude Desktop, or Claude Code) — for the planning session
- Session summaries from Step 2 (as file attachments or pasted context)
- Client website content (scraped via Firecrawl or similar — provides company context for personalisation)

**Time estimate:** 1–2 hours for planning, including iteration on the plan structure.

**Decision points:**
- What format should the deliverable take? (Static PDF, interactive app, Notion doc, etc.)
- Does the content need audience segmentation (multiple tracks)?
- What's the right level of depth? (Reference card vs comprehensive guide)
- What tools and skills should be included as starter kit materials?

**Tips:**
- Start a fresh Claude session for planning — do not continue from a session used for other work.
- Provide the session summaries as files, not pasted inline, to preserve context space.
- Ask Claude for options and recommendations before committing to a structure. "Given this training content and feedback, what format would best serve the client?" is a better starting prompt than "Write me a plan."
- Iterate on the plan. The first version will be broad — refine it by asking about specific sections, dependencies, and sequencing.

**Phew! example:** The session summaries, client feedback, initial thoughts document, and Phew! website scrape were loaded as context. Claude proposed the three-output structure (interactive playbook, starter kit, process doc), the two-track approach, and the phased build process. This plan was iterated over several sessions, producing the handoff document (`.planning/phew-follow-up-handoff.md`).

**Copyable prompt — Context loading and planning:**
```
I've just delivered AI training to [CLIENT NAME]. Attached are:
1. Session summary/transcript from the training
2. Client feedback received after the session
3. [Any other relevant context — website scrape, existing materials, etc.]

Based on the training content and client feedback, I need to create a structured follow-up deliverable. Please:
- Identify the key topics and pain points from the training
- Recommend a format for the deliverable (consider the client's technical level and team size)
- Propose a content structure with sections and suggested depth
- Flag any topics where the client explicitly requested more detail
- Suggest what starter materials (templates, prompts, config files) would be most valuable
```

### Step 4 — Produce Detailed Specs

**What happens:** Break the plan down into section-level specifications. Each spec must be self-contained — a build agent (human or AI) reading only that spec and its referenced files should have everything needed to produce the section.

**Tools:**
- **Claude** (or Claude Code) — for writing specs
- The plan from Step 3 as the guiding structure
- Source context documents (session summaries, research outputs) as references

**Time estimate:** 30–60 minutes per spec, depending on complexity. A 15-section playbook might take 1–2 days of spec writing.

**Tips:**
- Specs are the bridge between planning and building. Invest time here — a thorough spec prevents rework during the build.
- Each spec should define: purpose, source references, content outline, interaction design (if applicable), copyable content, acceptance criteria.
- Group specs by the agent that will build them. If using parallel agents, ensure no spec depends on output from another agent's work (or document the dependency explicitly).
- Run research tasks first if any specs depend on technical findings (e.g., which libraries to use, how a feature works). These research outputs become reference material for the specs.

**Phew! example:** Phase 0 ran seven research tasks in parallel (website scrape, UK English enforcement, command availability, brand voice workflow, context window mechanics, capabilities audit, app tech stack). Phase 1 then wrote 17 specs — one per playbook section, one for the starter kit, and one for this process document. Each spec referenced the relevant research outputs.

**Copyable prompt — Spec writing:**
```
I need to write a build spec for the following section of the deliverable:

[SECTION TITLE AND BRIEF DESCRIPTION]

The spec will be consumed by a build agent that works ONLY from this spec and referenced files. It must include:
- Purpose (what this section achieves)
- Source references (which files to read)
- Content outline (detailed breakdown of what to cover)
- Acceptance criteria (how to verify the section is complete)

Here is the overall plan for context: [ATTACH OR PASTE PLAN]
Here are the relevant source documents: [ATTACH SOURCES]
```

### Step 5 — Build with Parallel Agents

**What happens:** Execute the build using Claude Code, ideally with multiple agents working in parallel on different sections. Each agent receives its spec and works independently.

**Tools:**
- **Claude Code** — for building the deliverable (app code, content, configuration)
- **CoWork** — for orchestrating parallel agents (if available)
- Git — for version control and branch management
- The specs from Step 4 as each agent's brief

**Time estimate:** Varies by deliverable complexity. A React app with 15+ content sections might take 4–8 hours of agent time (wall-clock time is less with parallelism).

**Tips:**
- Set up the project shell first (scaffolding, routing, shared components) before dispatching content agents. Other agents need a working foundation to slot their output into.
- Each parallel agent gets a fresh 200k token context window — this is the key advantage. A single agent trying to build everything would degrade as context fills up.
- Use a CLAUDE.md file in the project root to give all agents shared context: tech stack, file structure, naming conventions, build commands.
- Review agent outputs as they complete. Fix integration issues early rather than accumulating them for a big merge.

**Phew! example:** Six parallel agents were allocated: (1) App shell and infrastructure, (2) Context and session management sections, (3) Skills, governance, and brand sections, (4) Developer track sections, (5) Starter kit files, (6) Welcome, process doc, and integration content. Each agent received its spec(s) from `.planning/specs/` and worked independently.

**Copyable prompt — Agent dispatch:**
```
You are a build agent for a client deliverable project. Your task is to build the section(s) defined in your spec.

Read the following spec carefully: [SPEC FILE PATH]
Read the project's CLAUDE.md for shared conventions: [CLAUDE.md PATH]

Build exactly what the spec describes. Do not add features beyond the spec. Do not modify files outside your designated output area unless the spec explicitly instructs you to.

When complete, verify your output against the acceptance criteria in the spec.
```

### Step 6 — Deploy the Deliverable

**What happens:** Integrate all agent outputs, run integration checks, and deploy the finished deliverable.

**Tools:**
- **Vercel** — for hosting interactive web apps (zero-config for Vite/React projects)
- **GitHub** — for the source repository
- **Claude Code** — for integration fixes and polish

**Time estimate:** 1–2 hours for integration and deployment.

**Tips:**
- Run a cross-reference checklist: does every item from the original plan appear in the final deliverable?
- Test all interactive elements: copy-to-clipboard buttons, navigation, responsive layout, keyboard accessibility.
- Verify tone consistency across sections built by different agents.
- Deploy to a preview URL first and review before promoting to production.

**Phew! example:** The React app was deployed to Vercel. The GitHub repo contained the app source, starter kit files, and this process document. A Vercel preview URL was shared for review before the final deployment.

### Step 7 — Bundle and Deliver

**What happens:** Package the deliverable with a clear delivery note explaining what is included, how to use it, and what to do next.

**Tools:**
- Email — for the delivery note
- The deployed app URL
- The starter kit (either downloadable from the app or as a repo folder)

**Time estimate:** 30 minutes for the delivery note.

**Tips:**
- Suggest a specific adoption order. Do not dump everything on the client and leave them to figure out what to do first. Recommend quick wins they can implement immediately.
- Offer a follow-up session. The deliverable often raises new questions once the team starts using it.
- Note which elements are reusable (e.g., governance policy template) — this positions future work naturally.

**Phew! example:** The delivery email included: the app URL, an explanation of the two-track structure, suggested first steps (start with context management, then set up CLAUDE.md files), how to use the feedback mechanism, and an offer for a follow-up session.

**Copyable prompt — Delivery note drafting:**
```
I need to write a brief delivery email to [CLIENT NAME] for the AI training follow-up materials I've prepared. The deliverable includes:

[LIST WHAT'S INCLUDED]

The email should:
- Explain what's included in plain language (no jargon)
- Suggest a practical adoption order (quick wins first)
- Explain how to provide feedback
- Offer a follow-up session
- Keep it under 300 words — respect their time
```

## Document Structure

The markdown file (`docs/repeatable-workflow.md`) should be organised as follows:

```
# Repeatable Workflow: Post-Training Follow-Up Deliverables

> Brief description: what this document is and who it's for.

## Overview
[1-paragraph summary of the 7-step process]

## The Process

### Step 1 — Record and Transcribe
### Step 2 — Summarise and Identify Focus Areas
### Step 3 — Provide Context to Claude and Plan
### Step 4 — Produce Detailed Specs
### Step 5 — Build with Parallel Agents
### Step 6 — Deploy the Deliverable
### Step 7 — Bundle and Deliver

## Adapting for Different Clients
[Scaling and customisation guidance]

## Template Prompts
[All copyable prompts collected in one place for quick reference]
```

### Formatting Conventions

- Use blockquote callouts (`> **Tip:**` or `> **Phew! example:**`) for tips and worked examples, visually distinguishing them from the main process instructions.
- Keep each step to roughly half a page. The entire document should not exceed 3 pages when rendered.
- Use bullet points for tools and tips, not dense paragraphs.
- All prompts should be in fenced code blocks (``` ```) so they render cleanly and are easy to copy.

## Copyable Content

The document contains four template prompts (one each for Steps 3, 4, 5, and 7). These are provided in the Content Outline above. Each prompt should be:

- In a fenced code block
- Written with `[PLACEHOLDER]` markers for client-specific values
- Immediately usable — a consultant should be able to copy, fill in the placeholders, and paste into Claude

## Meta-Narrative

This document is the strongest expression of the project's meta-narrative. The process it describes is the process that produced it. Make this explicit in the overview:

- The Phew! engagement is the worked example for every step.
- The tools described (Granola, Claude Code, Vercel) are the tools that were actually used.
- The parallel agent approach described in Step 5 is the same approach that built the playbook the reader may have just viewed.
- The document itself was written by one of those parallel agents (Agent 6), working from a spec — exactly as Step 4 describes.

Do not labour this point throughout the document. State it clearly once in the overview, then let the "Phew! example" callouts in each step demonstrate it naturally.

## Adapting for Different Clients

Include a brief section (5–10 bullet points) on how to adapt the process for different scenarios:

- **Simpler deliverables:** For a client who just needs a cheat-sheet, skip Steps 4 and 5 (no specs or parallel agents needed). Go from plan to a single Claude session producing the document.
- **Non-technical clients:** Replace the interactive app (Step 6) with a well-structured PDF or Notion page. The process steps remain the same; only the output format changes.
- **Larger teams:** For clients with multiple departments, consider more than two audience tracks. The spec-per-section approach (Step 4) scales naturally — add more specs and agents.
- **Ongoing engagements:** If the relationship extends beyond a single follow-up, add a feedback loop between Step 7 and Step 1 — the follow-up session becomes the next input.
- **Different tech stacks:** The Vite + React + Vercel stack used here can be swapped for any framework. The process is tool-agnostic; the workflow principles (record, summarise, plan, spec, build, deploy, deliver) remain constant.

## Acceptance Criteria

1. The file exists at `docs/repeatable-workflow.md` and is well-formed markdown.
2. All seven process steps are documented with: description, tools, time estimate, tips, and Phew! worked example.
3. Four template prompts are included (Steps 3, 4, 5, 7), each in a fenced code block with `[PLACEHOLDER]` markers.
4. The document is concise — no more than approximately 3 pages of rendered content (roughly 1,500–2,000 words).
5. The meta-narrative is stated clearly in the overview and demonstrated through Phew! examples, but not repeated excessively.
6. The "Adapting for Different Clients" section is present with at least 4 adaptation scenarios.
7. UK English is used throughout (spelling, grammar, currency references).
8. The document reads as a practical process guide — another consultant could follow it for a different client without needing to ask clarifying questions.
9. The document does not contain any Phew!-confidential information (no email addresses, no internal system details beyond what is already public).
10. The document is linked or referenced from within the app's navigation or welcome page so Phew! staff can view it.

## Build Agent Checklist

This spec produces a markdown document, not a React component. Most of the frontend checklist items do not apply directly. However, the document will be viewable within the app, so the following subset applies:

**UX Writing & Content**
- [ ] UK English throughout (spelling, grammar, currency, date format)
- [ ] Consistent terminology (no synonyms for the same action)
- [ ] Process steps are numbered and clearly sequenced
- [ ] Each step has: description, tools, time estimate, tips, worked example
- [ ] Template prompts use `[PLACEHOLDER]` markers for client-specific values
- [ ] All prompts are in fenced code blocks

**Content Quality**
- [ ] Document is self-contained — readable without other project files
- [ ] Meta-narrative stated once clearly, then demonstrated through examples
- [ ] Adapting section covers at least 4 different client scenarios
- [ ] Total length is approximately 1,500–2,000 words (2–3 rendered pages)
- [ ] No confidential client information included

**If rendered in the app (as a markdown view or dedicated page):**
- [ ] Body text >= 16px (1rem), using rem units
- [ ] Max line length: 65ch for body text
- [ ] All text meets WCAG AA contrast (4.5:1 body, 3:1 large/UI)
- [ ] Semantic HTML (headings, landmarks)
- [ ] Keyboard navigation functional
- [ ] Mobile-responsive layout
- [ ] Copy-to-clipboard on template prompt blocks
