# Spec 1.15 — Codebase Auditing & Technical Debt

> **Phase 2 build agent:** Agent 4 — Developer Track Sections
> **Track:** Developer

## Purpose

This section teaches Phew!'s developers how to use Claude as a codebase auditing tool — systematically analysing code for security issues, performance problems, architectural drift, and accumulated technical debt. It then covers the second half of the equation: using Claude to handle well-documented technical debt by generating fix plans, prioritising remediation, and executing targeted refactoring.

This section builds directly on section 1.10 (Codebase Mapping). The mapper's CONCERNS.md output is the starting point for auditing — it identifies problems. This section goes deeper: how to investigate those problems, assess their severity, and systematically address them. For Phew!, this is immediately practical — their WordPress and ASP.NET/C# projects will have accumulated technical debt, and Claude can help surface and resolve it without requiring the team to manually audit thousands of lines of code.

The key insight from the training: Claude's multi-file cross-reference analysis capabilities mean it can spot patterns and inconsistencies across an entire codebase that would take a human developer days to find manually.

## Source References

| File | Path | What to extract |
|------|------|-----------------|
| Training summary (Meeting 2) | `.planning/source-context/phew-training-claude-code-and-qa-summary.md` | Key references: "Multi-file cross-reference analysis", "Automatic architecture documentation generation", "Tech debt identification (21kb concerns file generated)", "Code analysis and debugging capabilities". These are direct quotes from the training — use them to connect this section back to what the team already heard. |
| Initial thoughts | `.planning/source-context/phew-initial-thoughts-for-meeting-follow-up.md` | Three relevant topics listed: "Agents to audit codebase", "Agents to handle any well-documented technical debt", and "Using agents to document and maintain codebase docs". This section addresses the first two directly. |
| CONCERNS.md template | `starter-kit/gsd-mapper/templates/codebase/concerns.md` | The template and good examples for the concerns document. The audit prompts in this section should target the same categories: tech debt, known bugs, security considerations, performance bottlenecks, fragile areas, scaling limits, dependencies at risk, missing features, test coverage gaps. |
| gsd-codebase-mapper agent | `starter-kit/gsd-mapper/agent/gsd-codebase-mapper.md` | The "concerns" focus area definition. Shows how the mapper agent explores for concerns: TODO/FIXME comments, large files, empty returns/stubs. The audit prompts in this section go deeper than this initial pass. |
| Doc structure source | `.planning/source-context/suggestions-related-to-doc-structure.md` | The concept of "doc-gardening agent" and mechanical enforcement of documentation quality. Connect to the idea that auditing is not a one-time activity but an ongoing process. Also references `tech-debt-tracker.md` as a living document. |
| App tech stack | `.planning/research/app-tech-stack.md` | UI component decisions: shadcn/ui, Shiki for syntax highlighting, copy-to-clipboard pattern. |
| Frontend skills review | `.planning/research/frontend-skills-review.md` | Design guidelines and the full Build Agent Checklist. |

## Content Outline

### Opening: Why Audit with AI?

A brief introduction (2-3 paragraphs) establishing the value proposition:

- Every codebase accumulates issues over time — shortcuts taken under deadline pressure, dependencies that fall behind, patterns that made sense years ago but no longer fit. For a team of Phew!'s size, dedicating time to comprehensive code audits is difficult to justify when there is client work to deliver.
- Claude changes this equation. It can analyse an entire codebase in minutes, cross-referencing across files to spot inconsistencies, security risks, and performance problems that are invisible when reviewing files one at a time. During the training, this was demonstrated when Claude generated a 21KB concerns file from a single codebase analysis.
- This section covers two complementary workflows: **auditing** (finding problems) and **remediation** (fixing them). The mapper's CONCERNS.md from section 1.10 is your starting point — this section takes you deeper.

### Part 1: Codebase Auditing

#### The Audit Workflow

Explain the general approach to running an AI-powered codebase audit:

1. **Start with the mapper output.** If you have not already run `/gsd:map-codebase` (section 1.10), do that first. The CONCERNS.md document gives you a baseline understanding of known issues.

2. **Choose your audit focus.** Auditing everything at once produces unfocused results. Pick a specific focus area for each audit session:
   - Security audit
   - Performance audit
   - Dependency health audit
   - Architecture consistency audit
   - Test coverage audit
   - Code quality / standards compliance audit

3. **Run a focused audit prompt.** Use one of the prompts below, tailored to the specific focus area. Each prompt instructs Claude to systematically explore, analyse, and report.

4. **Review and triage findings.** Claude will produce a structured report. Review each finding, confirm it is valid (Claude can produce false positives), and prioritise.

5. **Document confirmed findings.** Add confirmed issues to your CONCERNS.md or tech debt tracker. This creates the "well-documented technical debt" that enables the remediation workflow.

#### Multi-File Cross-Reference Analysis

Explain what makes Claude particularly effective at auditing — the ability to analyse patterns across files:

- **Inconsistent patterns:** Claude can scan all files in a directory and identify where coding conventions are not followed — e.g., error handling that differs between controllers, naming patterns that vary, or import styles that are inconsistent.
- **Dead code detection:** By tracing imports and references across the codebase, Claude can identify exported functions that are never imported, components that are never rendered, and routes that are never called.
- **Dependency analysis:** Claude can read `package.json` (or equivalent), check which dependencies are actually imported in source code, and flag unused dependencies or version mismatches.
- **Security pattern scanning:** Claude can look for common vulnerability patterns across all files simultaneously — hardcoded credentials, SQL injection risks, missing input validation, exposed API endpoints without authentication.

**Context management note:** For large codebases, a single audit prompt may not cover everything. Use the subagent pattern from section 1.10 — break the audit into focused tasks, each with its own context window. Alternatively, direct Claude to specific directories or file types.

#### Audit Prompts by Focus Area

Present each audit prompt as a copyable code block with a brief explanation of what it does and what to expect in the output.

**Security Audit Prompt:**
```
Conduct a security audit of this codebase. For each finding, provide:
- Severity: Critical / High / Medium / Low
- File path(s) affected
- Description of the vulnerability
- How it could be exploited
- Recommended fix

Focus areas:
1. Hardcoded secrets, API keys, or credentials in source code
2. SQL injection or NoSQL injection risks
3. Cross-site scripting (XSS) vulnerabilities
4. Missing authentication or authorisation checks on endpoints
5. Insecure file upload handling
6. Missing input validation or sanitisation
7. Insecure dependency versions with known CVEs
8. Exposed debug/development endpoints in production configuration
9. Missing CSRF protection
10. Insecure session management

Start by scanning all source files, then check configuration files and environment handling. Report findings in severity order (critical first).
```

**Performance Audit Prompt:**
```
Analyse this codebase for performance issues. For each finding, provide:
- Impact: High / Medium / Low
- File path(s) affected
- Description of the bottleneck
- Estimated impact on user experience or server resources
- Recommended optimisation

Focus areas:
1. N+1 query patterns (database calls inside loops)
2. Missing database indexes (queries filtering on unindexed columns)
3. Unbounded queries (no pagination or limit)
4. Synchronous operations that should be async
5. Large files loaded entirely into memory
6. Missing caching opportunities
7. Redundant API calls or database queries
8. Unoptimised images or assets
9. Render-blocking resources
10. Missing lazy loading for components or routes

Check database queries, API endpoints, and rendering logic. Prioritise findings by user-facing impact.
```

**Dependency Health Audit Prompt:**
```
Audit the dependencies in this project. For each issue found, provide:
- Risk level: Critical / High / Medium / Low
- Package name and current version
- Issue description
- Recommended action

Check for:
1. Dependencies with known security vulnerabilities (check against recent CVE databases you're aware of)
2. Deprecated or unmaintained packages (no updates in 12+ months)
3. Major version upgrades available that may include breaking changes
4. Dependencies imported in package.json but never used in source code
5. Dependencies used in source code but not declared in package.json
6. Duplicate dependencies (same functionality from multiple packages)
7. Dependencies that could be replaced with built-in language features
8. Packages with restrictive or changing licences
9. Pinned versions that should use ranges (or ranges that should be pinned)
10. Dev dependencies incorrectly listed as production dependencies (or vice versa)

Read the package manifest and lock file, then cross-reference with actual imports in the source code.
```

**Architecture Consistency Audit Prompt:**
```
Analyse this codebase for architectural consistency. Compare the actual code against the patterns described in the documentation (check CLAUDE.md, any docs/ folder, and ARCHITECTURE.md if present). For each finding, provide:
- Severity: High / Medium / Low
- File path(s) affected
- Description of the inconsistency
- What the intended pattern is
- Recommended fix

Check for:
1. Files in the wrong directory (based on the project's stated structure)
2. Layer violations (e.g., UI code directly accessing the database)
3. Circular dependencies between modules
4. Inconsistent error handling patterns across similar components
5. Mixed naming conventions (camelCase vs snake_case in the same layer)
6. Business logic in controllers/routes instead of services
7. Missing abstraction layers (direct third-party SDK calls scattered across multiple files)
8. God objects or god files (single files with too many responsibilities)
9. Inconsistent API response formats
10. Configuration scattered across multiple files instead of centralised

Compare the documented architecture (if any) against the actual code organisation. If no architecture documentation exists, infer the intended patterns from the majority of the code and flag deviations.
```

**Test Coverage Audit Prompt:**
```
Analyse the test coverage and testing practices in this codebase. For each finding, provide:
- Priority: High / Medium / Low
- File path(s) affected
- Description of the gap
- Risk if untested code fails in production
- Suggested test approach

Check for:
1. Source files with no corresponding test file
2. Critical business logic without test coverage
3. API endpoints without integration tests
4. Error paths that are never tested (catch blocks, error handlers)
5. Edge cases not covered (empty arrays, null values, boundary conditions)
6. Mocked tests that do not reflect real behaviour
7. Tests that always pass regardless of implementation (tautological tests)
8. Missing test fixtures or factories (tests creating complex objects inline)
9. Flaky tests (tests that depend on timing, order, or external state)
10. Missing E2E tests for critical user flows

Start by listing all source files and their corresponding test files (if any). Then analyse the test files for quality and coverage depth.
```

**Code Quality / Standards Compliance Prompt:**
```
Review this codebase for code quality and standards compliance. For each finding, provide:
- Priority: High / Medium / Low
- File path(s) affected
- Description of the issue
- Recommended improvement

Check for:
1. Functions exceeding 50 lines (candidates for extraction)
2. Deeply nested conditionals (more than 3 levels)
3. Magic numbers or strings without named constants
4. Copy-pasted code blocks (duplicated logic)
5. Missing error handling (unhandled promise rejections, missing try/catch)
6. Console.log statements that should be proper logging or removed
7. Commented-out code that should be deleted
8. TODO/FIXME comments without associated issue tracker references
9. Missing JSDoc/TSDoc on public functions
10. Inconsistent use of async/await vs promises vs callbacks

Scan all source files (not test files or config files). Group findings by file and prioritise by impact on maintainability.
```

### Part 2: Handling Technical Debt

#### From Audit to Action

Explain the bridge between finding problems and fixing them:

- An audit produces findings. Findings need to be triaged, documented, and prioritised before any code changes happen.
- The worst outcome is a long list of issues that never gets addressed. The goal is a short, prioritised list of actions that the team can work through incrementally.
- Claude can help with both the prioritisation and the execution — but the team makes the final decisions about what to fix and when.

#### Documenting Technical Debt

Explain the documentation pattern that makes technical debt actionable:

**The principle:** Claude can only act on well-documented debt. A vague note like "auth needs work" is not actionable. A structured entry with file paths, impact assessment, and suggested fix approach gives Claude everything it needs to generate a fix plan or implement the change directly.

**Recommended format (from CONCERNS.md template):**
```markdown
**[Area/Component]:**
- Issue: [What is the shortcut/workaround]
- Files: `[file paths]`
- Why: [Why it was done this way]
- Impact: [What breaks or degrades because of it]
- Fix approach: [How to properly address it]
```

**Example for a Phew! project:**
```markdown
**Manual webhook signature validation:**
- Issue: Copy-pasted Stripe webhook verification code in 3 different endpoints
- Files: `app/api/webhooks/stripe/route.ts`, `app/api/webhooks/checkout/route.ts`, `app/api/webhooks/subscription/route.ts`
- Why: Each webhook was added ad-hoc without abstraction
- Impact: Easy to miss verification in new webhooks (security risk)
- Fix approach: Create shared `lib/stripe/validate-webhook.ts` middleware
```

#### Prioritisation

Explain a practical prioritisation approach:

**The prioritisation prompt:**
```
I have the following technical debt items documented in my codebase. Review each one and prioritise them using this framework:

Priority criteria:
1. Security risk (could it expose data or create vulnerabilities?)
2. User impact (does it affect end users directly?)
3. Developer impact (does it slow down future development?)
4. Fix complexity (how much effort to resolve?)
5. Blast radius (how many files/systems does the fix touch?)

For each item, assign:
- Priority: P1 (fix now) / P2 (fix this sprint) / P3 (fix this quarter) / P4 (fix when convenient)
- Rationale: 1-2 sentences explaining the priority
- Suggested approach: Brief outline of how to fix it
- Estimated effort: Small (< 1 hour) / Medium (1-4 hours) / Large (4+ hours)

Group the output by priority level.

Here are the items:
[paste your CONCERNS.md content or audit findings here]
```

#### Executing Debt Remediation

Explain how to use Claude to actually fix documented technical debt:

**The remediation prompt pattern:**
```
I need to address the following technical debt item:

[paste the documented debt entry from CONCERNS.md]

Before making any changes:
1. Read all affected files listed above
2. Understand the current implementation and its dependencies
3. Propose a fix plan with:
   - Files to modify
   - Files to create (if any)
   - Files to delete (if any)
   - Order of changes (to avoid breaking intermediate states)
   - Tests to add or update
4. Wait for my approval before implementing

Important constraints:
- Follow the existing coding conventions in this project
- Do not change any public APIs unless the fix requires it
- Add or update tests for any changed behaviour
- If the fix is larger than expected, flag it and suggest a phased approach
```

**Key points to emphasise:**

1. **Always review before executing.** Claude should propose the fix plan before implementing it. This prevents over-engineering and ensures the team agrees with the approach.

2. **One debt item at a time.** Do not try to fix multiple technical debt items in a single session. Each item gets its own focused session with a clean context.

3. **Test after every fix.** Run existing tests after each change to confirm nothing broke. If the codebase has no tests, add tests for the changed code as part of the fix.

4. **Update the documentation.** After fixing a debt item, update CONCERNS.md to remove the resolved entry. If the fix introduced new patterns, update CONVENTIONS.md.

#### Ongoing Maintenance: The Doc-Gardening Pattern

Connect to the maintenance concept from the OpenAI/Reddit source:

- Auditing is not a one-time activity. Codebases continue to accumulate debt as new features are added and requirements change.
- Establish a cadence: run the mapper (`/gsd:map-codebase`) quarterly, or before any major new feature begins.
- Keep CONCERNS.md as a living document — update it when issues are found and resolved.
- Consider the "doc-gardening agent" pattern: a scheduled task that scans for stale or obsolete documentation and flags it for review. This connects to section 1.7 (Recurring & Scheduled Tasks).

### Automatic Architecture Documentation Generation

Briefly explain this capability, referenced in the training:

- Claude can generate architecture documentation from code analysis — this is what the mapper's ARCHITECTURE.md does.
- Beyond the initial mapping, Claude can keep architecture documentation current by comparing the documented architecture against the actual code and flagging drift.
- This is particularly valuable after a period of rapid development where the architecture may have evolved beyond what the documentation describes.

**Architecture drift detection prompt:**
```
Compare the architecture described in docs/architecture/ (or .planning/codebase/ARCHITECTURE.md) against the actual codebase. For each discrepancy, provide:
- What the documentation says
- What the code actually does
- Which is correct (documentation is outdated, or code has drifted from the intended architecture)
- Recommended action (update docs, refactor code, or accept the change)

Focus on:
1. Layers and their actual dependencies vs documented dependencies
2. File/directory organisation vs documented structure
3. Data flow patterns vs documented flows
4. Error handling strategy vs documented strategy
5. Naming conventions vs documented conventions
```

## Interaction Design

### Components Used

| Component | Source | Usage |
|-----------|--------|-------|
| `Accordion` | shadcn/ui | Audit prompts by focus area — each type as a collapsible item |
| `Card` | shadcn/ui | Workflow overview steps, prioritisation framework display |
| `Alert` | shadcn/ui | Key principle callouts, security warnings, "always review before executing" emphasis |
| `Badge` | shadcn/ui | Priority labels (P1, P2, P3, P4), severity indicators (Critical, High, Medium, Low), effort estimates |
| `Separator` | shadcn/ui | Visual dividers between Part 1 (Auditing) and Part 2 (Technical Debt) |
| `Tabs` | shadcn/ui | Could be used to switch between "Auditing" and "Remediation" views if the content benefits from it, but a single scrollable page is likely better for this content |
| `CodeBlock` (custom) | Shiki via react-shiki | Syntax-highlighted code blocks for all prompts, documentation format examples, and command examples |
| `CopyButton` (custom) | Native Clipboard API | Copy button on every prompt, template, and code block |

### Layout

- **Mobile (< 640px):** Single column. Audit prompts in accordion are fully collapsible. Code blocks use full width with horizontal scroll for long lines. Priority badges stack within cards.
- **Tablet (640px-1023px):** Single column with wider content area. Code blocks display more comfortably.
- **Desktop (1024px+):** Single column content with max-width 65ch for body text. Code blocks can extend beyond the text column. The prioritisation framework could use a two-column layout (criteria on left, examples on right) if the design supports it.

### Key Interactions

1. **Audit prompt accordion:** Each of the 6 audit focus areas is an AccordionItem. The trigger shows the audit type (e.g., "Security Audit") and a one-line description. Expanding reveals the explanation and the full copyable prompt. Default: all collapsed. Mode: single-expand recommended to keep the page manageable.
2. **Code blocks:** All prompts are syntax-highlighted with Shiki (language: `text` or `markdown`) and include a copy button positioned in the top-right corner. Prompts are substantial (10-20 lines each), so the copy button is essential — users should not have to manually select and copy.
3. **Priority badges:** Use colour-coded Badge components for priority levels: P1 (red/destructive variant), P2 (amber/warning), P3 (blue/default), P4 (grey/muted). These reinforce the prioritisation framework visually.

### Animation / Motion

- **Accordion expand/collapse:** Use shadcn's built-in Radix animation. Respect `prefers-reduced-motion`.
- **No other animations.** This section is a practical reference — users will return to copy specific prompts. Animations should not slow down repeat access.

## Copyable Content

All of the following must have a copy-to-clipboard button:

### Audit Prompts (6 total)
1. Security Audit Prompt
2. Performance Audit Prompt
3. Dependency Health Audit Prompt
4. Architecture Consistency Audit Prompt
5. Test Coverage Audit Prompt
6. Code Quality / Standards Compliance Prompt

### Documentation Templates
7. Technical debt documentation format (the structured CONCERNS.md entry format)
8. Example technical debt entry (the Stripe webhook example)

### Remediation Prompts
9. Prioritisation prompt
10. Remediation prompt pattern (the "fix this debt item" prompt)
11. Architecture drift detection prompt

**Total copyable blocks: 11.** Each must have a visible copy button and the standard "Copied!" feedback animation.

## Two-Track Considerations

This section is **Developer track only**. It assumes the reader:
- Has access to Claude Code (terminal or IDE)
- Understands concepts like technical debt, code review, test coverage, and security vulnerabilities
- Is comfortable running audit prompts and reviewing technical output
- Has worked with the codebase they are auditing (or has run the mapper from section 1.10 first)

**General track users** should not see this section in the navigation. The concepts (AI can help find and fix problems in code) could be summarised in a single sentence on the general track landing page if a cross-reference is needed, but the detailed prompts and workflows are developer-only content.

**Cross-references within developer track:**
- Section 1.10 (Codebase Mapping) — the mapper's CONCERNS.md output is the starting point for auditing. Reference this explicitly and recommend running the mapper first.
- Section 1.9 (Documentation Structure) — the doc-gardening pattern connects to maintaining the `/docs` structure. CONCERNS.md is one of the maintained documents.
- Section 1.11 (Avoiding Hallucinations) — the "always review before executing" principle and the "propose a plan, wait for approval" pattern are specific applications of the anti-hallucination techniques.
- Section 1.8 (CLAUDE.md) — after fixing technical debt, updating the CLAUDE.md and documentation is part of the workflow.

## Acceptance Criteria

1. The section clearly explains the value of AI-powered codebase auditing for a small team (Phew!'s context: 9-11 staff, cannot dedicate weeks to manual audits).
2. The audit workflow is presented as a clear step-by-step process: start with mapper output, choose focus, run audit prompt, review findings, document confirmed issues.
3. Multi-file cross-reference analysis is explained with concrete examples of what it can detect (inconsistent patterns, dead code, dependency issues, security patterns).
4. All 6 audit focus area prompts are provided as substantial, ready-to-use copyable blocks. Each prompt includes: what to check (10 specific items), what output format to expect (structured findings with severity/priority, file paths, descriptions, and recommendations).
5. The technical debt documentation format is explained with a template and a concrete example (contextualised for a Phew!-like project).
6. The prioritisation prompt uses a clear framework (5 criteria) and produces actionable output (P1-P4 levels with rationale, approach, and effort estimate).
7. The remediation prompt pattern includes the safety guardrail: propose plan first, wait for approval before implementing.
8. The "one debt item at a time" and "test after every fix" principles are explicitly stated.
9. The doc-gardening / ongoing maintenance pattern is described, connecting to the recurring tasks concept from section 1.7.
10. Architecture drift detection is covered with a copyable prompt.
11. The connection to section 1.10 (Codebase Mapping) is explicitly made: mapper's CONCERNS.md is the starting point, audit prompts go deeper.
12. All 11 copyable content blocks have copy-to-clipboard buttons with visual feedback.
13. Content is contextualised for Phew!: references to WordPress and ASP.NET/C# projects, SMB-appropriate language, practical rather than theoretical.
14. UK English is used throughout.
15. The section loads correctly within the developer track navigation and does not depend on any other section being rendered.
16. Semantic HTML is used: proper heading hierarchy, accordion uses `aria-expanded`/`aria-controls`, code blocks are accessible.
17. Priority badges use distinct colours/variants that meet WCAG AA contrast requirements and do not rely on colour alone (labels are always present alongside colour).

## Build Agent Checklist

### Frontend Quality Checklist

**Typography**
- [ ] Body text >= 16px (1rem), using rem units
- [ ] Max line length: 65ch for body text
- [ ] Fluid type (clamp) on headings; fixed sizes on UI controls
- [ ] Font stacks include size-adjusted fallback
- [ ] No generic fonts (Inter, Roboto, etc.) unless explicitly specified

**Colour & Theming**
- [ ] All custom colours defined in OKLCH via CSS variables
- [ ] Neutrals tinted towards brand hue (not pure grey)
- [ ] No pure black (#000) or pure white (#fff)
- [ ] Dark mode tested and functional (if applicable to section)
- [ ] All text meets WCAG AA contrast (4.5:1 body, 3:1 large/UI)

**Layout & Spacing**
- [ ] Spacing values from the 4pt grid (4, 8, 12, 16, 24, 32, 48, 64, 96)
- [ ] Visual hierarchy uses 2+ dimensions (size, weight, colour, space)
- [ ] No nested cards
- [ ] No identical repeating card grids

**Motion & Animation**
- [ ] Only `transform` and `opacity` animated (no width/height/top/left)
- [ ] Timing follows 100/300/500 rule
- [ ] Easing uses exponential curves (not default `ease` or bounce)
- [ ] `prefers-reduced-motion` handled (crossfade fallback or disable)
- [ ] Exit animations faster than entrances
- [ ] Motion used via Tailwind transitions where CSS suffices; Motion library only for layout/enter/exit

**Interaction**
- [ ] All 8 interactive states designed (default, hover, focus, active, disabled, loading, error, success)
- [ ] `:focus-visible` ring on all interactive elements (2-3px, offset, 3:1 contrast)
- [ ] Touch targets >= 44px
- [ ] Copy-to-clipboard on every prompt/template/code block
- [ ] Skeleton screens for loading states (not generic spinners)

**Accessibility**
- [ ] Semantic HTML (headings, landmarks, labels)
- [ ] Skip link present
- [ ] Keyboard navigation tested (Tab, Enter, Escape, Arrows)
- [ ] `aria-expanded`, `aria-controls` on accordions/collapsibles
- [ ] `aria-label` on icon-only buttons
- [ ] Images have `alt` text (or `alt=""` for decorative)
- [ ] Never `outline: none` without `:focus-visible` replacement

**Performance**
- [ ] No barrel file imports (direct imports from source)
- [ ] Shiki lazy-loaded (not in initial bundle)
- [ ] Derived state computed during render (no useEffect for derived values)
- [ ] `content-visibility: auto` on long scrollable content
- [ ] Passive event listeners on scroll/touch handlers
- [ ] Static JSX hoisted outside components where possible

**Responsive**
- [ ] Mobile-first (base styles for mobile, min-width queries for larger)
- [ ] Tested at 320px, 640px, 768px, 1024px widths
- [ ] No critical functionality hidden on mobile
- [ ] Hover-dependent features have touch alternatives
- [ ] Safe area insets handled for mobile

**UX Writing & Content**
- [ ] UK English throughout (spelling, grammar, currency, date format)
- [ ] Button labels use verb + object pattern
- [ ] Error messages answer: what, why, how to fix
- [ ] Consistent terminology (no synonyms for the same action)
- [ ] Empty states are actionable, not just "Nothing here"

**Code Quality**
- [ ] Tailwind utility classes only (no CSS modules or styled-components)
- [ ] `cn()` utility for conditional class merging
- [ ] `@/` path alias for all imports
- [ ] Components composed from shadcn primitives (not mega-components)
- [ ] Content defined as typed TS objects, not hard-coded JSX
