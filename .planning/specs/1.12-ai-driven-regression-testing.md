# Spec 1.12 — AI-Driven Regression Testing

> **Phase 2 build agent:** Agent 4 — Developer Track Sections
> **Track:** Developer

## Purpose

This section addresses a specific opportunity raised during the Claude Code and QA training session: using AI-driven browser automation to complement or eventually replace Phew!'s current use of Ghost Inspector for regression testing. Ghost Inspector is Phew!'s existing tool for automated browser-based testing — the section must respect this context and position AI-driven testing as a complementary evolution, not an immediate rip-and-replace.

The section must be honest about current limitations. AI-driven browser testing is real and evolving rapidly, but the tooling is not yet a mature, drop-in replacement for dedicated regression testing platforms. The section should give Phew!'s developers a clear picture of what is possible today, what is coming soon, and a practical adoption path that starts with low-risk experiments alongside their existing Ghost Inspector setup.

Key topics: the current state of AI browser automation (CoWork, Playwright MCP, computer use), how these capabilities compare to Ghost Inspector's feature set, integration approaches, and concrete starting points for Phew!.

## Source References

| File | Path | What to extract |
|------|------|-----------------|
| Initial thoughts | `.planning/source-context/phew-initial-thoughts-for-meeting-follow-up.md` | "AI-driven regression testing, to compliment or replace use of Ghost Inspector" — the original requirement. Also mentions "agent-browser" as a third-party tool and the agent harness concept. |
| Training summary (dev session) | `.planning/source-context/phew-training-claude-code-and-qa-summary.md` | "Regression testing automation — browser-based testing workflows, integration with existing tools or custom solutions, potential Ghost Inspector replacement." Also: "CoWork enables direct browser manipulation — orange hue indicates AI control — can handle login, navigation, form filling." And: "Computer use extensions for broader application control — script generation for complex workflows." |
| App tech stack | `.planning/research/app-tech-stack.md` | UI component decisions: shadcn/ui (Card, Accordion, Alert, Badge, Tabs), copy-to-clipboard pattern, Shiki syntax highlighting via react-shiki. |
| Frontend skills review | `.planning/research/frontend-skills-review.md` | Design guidelines, accessibility, interaction states, responsive design, and the full Build Agent Checklist (copied in full at the end of this spec). |
| Handoff doc | `.planning/phew-follow-up-handoff.md` | Section 1.12 definition, Agent 4 assignment, coverage checklist mapping. |

## Content Outline

### Opening: The Testing Landscape is Shifting

A brief (2-3 paragraph) introduction that frames the topic:

- Regression testing has traditionally required dedicated tools that record or script browser interactions and replay them. Ghost Inspector is a solid example of this — and it works well for Phew!'s current needs.
- AI is changing this landscape. Instead of brittle recorded scripts that break when a CSS class changes, AI-driven testing can understand what a page should do and verify it at a semantic level. This is not science fiction — it is available now, with caveats.
- This section lays out what is currently possible, how it compares to Ghost Inspector, and a practical path for Phew! to start experimenting alongside their existing setup.

**Tone:** Measured and honest. Do not oversell. Acknowledge that Ghost Inspector works and has served Phew! well. The case for AI-driven testing is about long-term capability, not immediate replacement.

### Current AI Testing Capabilities

Present the three main approaches currently available, each in its own subsection:

#### 1. CoWork Browser Automation

- **What it is:** Anthropic's CoWork (formerly Claude artifacts + computer use) can directly control a web browser. When CoWork is in control, the browser displays an orange hue to indicate AI operation.
- **What it can do today:**
  - Navigate to URLs and interact with page elements (click, type, scroll)
  - Handle authentication flows (login, SSO)
  - Fill forms and submit data
  - Take screenshots and verify visual state
  - Execute multi-step user journeys
- **Limitations:**
  - Requires manual session initiation (not yet schedulable as a cron job)
  - No built-in assertion framework — you describe expected outcomes in natural language
  - Speed: slower than headless browser automation because it operates through the visual interface
  - Reliability: AI visual understanding can occasionally misidentify elements, especially on complex pages
  - No native CI/CD integration
- **Best for:** Exploratory testing, ad-hoc verification, testing flows that are hard to script (e.g., visually complex interfaces, dynamic content)

#### 2. Playwright MCP (Claude Code Integration)

- **What it is:** The Playwright MCP server gives Claude Code direct access to browser automation through Microsoft's Playwright framework — the same engine behind many professional testing tools.
- **What it can do today:**
  - Launch browsers (Chromium, Firefox, WebKit) and navigate to pages
  - Interact with elements using robust selectors (CSS, XPath, text content, ARIA roles)
  - Assert page state programmatically (element visibility, text content, URL)
  - Generate Playwright test scripts that can be saved and re-run independently
  - Run headlessly in CI/CD pipelines
- **Limitations:**
  - Requires Claude Code (not available in claude.ai or Claude Desktop)
  - Claude generates the test scripts — hallucinated selectors are a real risk (cross-reference section 1.11)
  - Generated tests need human review before being trusted in CI/CD
  - No built-in visual regression (screenshot comparison) without additional tooling
- **Best for:** Generating test scripts for known user flows, bootstrapping a Playwright test suite, testing during development

#### 3. Computer Use API

- **What it is:** Anthropic's computer use capability allows Claude to control a full desktop environment — mouse, keyboard, screenshots. Available through the API.
- **What it can do today:**
  - Control any desktop application, not just browsers
  - Perform complex multi-application workflows
  - Interact with applications that do not have APIs (e.g., legacy desktop software)
- **Limitations:**
  - API-only (requires development work to set up)
  - Significantly more expensive per action than scripted automation
  - Slower than direct browser automation
  - Not designed for high-frequency regression testing
- **Best for:** One-off automation tasks, testing legacy applications, cross-application workflows. Not a primary regression testing tool.

### Ghost Inspector Comparison

Present a comparison table that honestly maps Ghost Inspector's features against AI-driven alternatives. This helps Phew!'s developers understand what they would keep, what they would gain, and what gaps remain.

| Capability | Ghost Inspector | AI-Driven (Current State) |
|------------|----------------|--------------------------|
| **Recorded test creation** | Built-in browser extension recorder | CoWork can follow instructions to navigate; Playwright MCP can generate scripts from descriptions |
| **Scheduled execution** | Yes — cron-like scheduling, recurring runs | Not natively available yet. Would require a custom orchestration layer |
| **CI/CD integration** | Built-in (webhooks, API, GitHub Actions) | Playwright scripts can run in CI/CD; CoWork/computer use cannot |
| **Visual regression** | Screenshot comparison built-in | Not built-in. Would need Percy, Playwright screenshot comparison, or similar |
| **Element selectors** | CSS selectors (can break on redesign) | AI can adapt to layout changes; Playwright selectors are more robust than CSS |
| **Authentication handling** | Cookie injection, API-based auth | CoWork can perform actual login flows; Playwright can handle auth state |
| **Reporting & history** | Dashboard with pass/fail history, screenshots | No built-in reporting. Would need custom solution |
| **Team collaboration** | Shared tests, team management | Tests are code — version-controlled and reviewable in Git |
| **Self-healing tests** | No (tests break when selectors change) | AI-generated tests can be regenerated from natural-language descriptions |
| **Cost** | Subscription-based per test run | API token costs per generation; Playwright execution is free |
| **Maintenance burden** | High — tests break frequently on redesign | Lower for AI-generated tests (regenerate from descriptions); higher for initial setup |

**Key insight to emphasise:** The most compelling advantage of AI-driven testing is not speed or cost — it is **self-healing**. Traditional regression tests are notoriously brittle. A CSS class rename, a layout restructure, or a component refactor breaks tests even when functionality is unchanged. AI-generated tests from natural-language descriptions can be regenerated when the UI changes, rather than manually fixed.

### Integration Approaches

Present two practical approaches for Phew!, from conservative to progressive:

#### Approach A: AI-Assisted Test Generation (Conservative)

Use Claude (via Claude Code with Playwright MCP) to generate Playwright test scripts, then run and maintain those scripts using standard tooling.

**How it works:**
1. Describe each test scenario in natural language
2. Claude generates a Playwright test script
3. Review the script, fix any hallucinated selectors
4. Commit the test to your repo
5. Run via standard Playwright CLI in CI/CD
6. When tests break after a redesign, ask Claude to regenerate from the original description

**Advantages:** Low risk, integrates with standard CI/CD, tests are versioned code, Ghost Inspector stays in place for critical tests.

**Phew! starting point:** Pick 3-5 existing Ghost Inspector tests and recreate them as Playwright tests using Claude. Compare reliability and maintenance burden over 2-3 months.

#### Approach B: AI-as-Tester Hybrid (Progressive)

Use CoWork or Playwright MCP for exploratory and ad-hoc testing alongside Ghost Inspector for critical regression paths.

**How it works:**
1. Ghost Inspector continues handling critical regression suites (login, checkout, key user journeys)
2. CoWork handles exploratory testing: "Navigate to the LMS admin panel, create a new training module, assign it to a test user, and verify the user can access it"
3. For new features, use Claude to generate Playwright tests before the feature reaches Ghost Inspector
4. Gradually migrate Ghost Inspector tests to Playwright as confidence builds

**Advantages:** No disruption to existing testing, AI handles the types of testing Ghost Inspector is worst at (exploratory, complex multi-step flows), gradual migration path.

**Phew! starting point:** Start using CoWork for manual QA tasks that are currently done by hand (new feature walkthroughs, cross-browser checks). Document which tasks it handles well and which it struggles with.

### Practical Starting Points for Phew!

A prioritised list of concrete first steps, presented as a numbered "getting started" guide:

1. **Install the Playwright MCP** — If not already available, add the Playwright MCP server to your Claude Code configuration. This gives Claude the ability to launch browsers and interact with your local dev environment. (Cross-reference section 1.13 — Safe MCP Usage for installation guidance.)

2. **Write your first AI-generated test** — Pick a simple, stable user flow (e.g., "log into the LMS admin panel and verify the dashboard loads"). Ask Claude to generate a Playwright test for it. Review the output, run it, and see how it compares to the equivalent Ghost Inspector test.

3. **Build a natural-language test catalogue** — Before generating any more tests, write plain-English descriptions of your 10 most important user journeys. These descriptions become the "source of truth" for test regeneration. Store them in your repo (e.g., `/docs/test-scenarios/`).

4. **Experiment with CoWork for exploratory testing** — On your next feature release, use CoWork to walk through the new functionality instead of (or in addition to) manual testing. Note where it succeeds and where it struggles.

5. **Evaluate after 4-6 weeks** — After running AI-generated Playwright tests alongside Ghost Inspector for a month or two, assess: Which approach caught more bugs? Which required less maintenance? Where did each fail?

6. **Do not cancel Ghost Inspector yet** — Keep it running for your critical paths until you have confidence in the replacement. The goal is not to save the subscription cost — it is to have better tests.

### Honest Limitations and Caveats

A clearly-marked callout (Alert component) that addresses limitations head-on:

- **AI testing is not deterministic.** The same prompt can generate slightly different test scripts on different runs. This matters for CI/CD reliability.
- **Hallucinated selectors are a real risk.** Claude may generate selectors that look correct but do not match your actual DOM. Always review generated tests manually. (Cross-reference section 1.11.)
- **Scheduling is not solved.** Ghost Inspector can run tests on a schedule. AI-driven testing currently requires manual initiation or custom orchestration.
- **Reporting is DIY.** Ghost Inspector provides dashboards, history, and team visibility. AI-generated Playwright tests require you to build or adopt a reporting layer (Playwright's built-in HTML reporter is a reasonable starting point).
- **Cost can be unpredictable.** Ghost Inspector has predictable subscription costs. AI token costs depend on test complexity and how often you regenerate tests.
- **This landscape is changing fast.** Specific tool capabilities described here may change within months. The principles (AI-assisted generation, self-healing tests, natural-language descriptions) are more durable than the specific tooling.

## Interaction Design

### Components Used

| Component | Source | Usage |
|-----------|--------|-------|
| `Card` | shadcn/ui | Each testing approach (CoWork, Playwright, Computer Use) in its own card with capabilities and limitations. |
| `Tabs` | shadcn/ui | The two integration approaches (Conservative / Progressive) presented as tabs, allowing comparison. |
| `Alert` | shadcn/ui | "Honest Limitations" callout. Variant: warning/caution style. |
| `Badge` | shadcn/ui | Maturity indicators on each approach: "Available now", "Early stage", "API only". |
| `Accordion` | shadcn/ui | The "Getting Started" steps can be collapsed/expanded. |
| `Button` | shadcn/ui | Copy-to-clipboard buttons on all prompt examples. |
| `Separator` | shadcn/ui | Between major sections (capabilities, comparison, approaches, getting started). |
| `Tooltip` | shadcn/ui | On maturity badges to provide additional context. |

### Comparison Table

The Ghost Inspector comparison table should be implemented as a responsive HTML table:
- **Desktop:** Full table with all columns visible.
- **Mobile:** Either a horizontally scrollable table within a `ScrollArea`, or a stacked card layout where each row becomes a card with "Ghost Inspector: X" / "AI-Driven: Y" pairs.

Use subtle row striping (Tailwind `even:bg-muted/50`) for readability. Highlight the "self-healing" row as the key differentiator.

### Layout

- **Mobile (< 640px):** Single column. Approach cards stack. Comparison table scrolls horizontally or stacks. Code blocks scroll horizontally.
- **Tablet (640px-1023px):** Single column, wider. Comparison table fits without scroll.
- **Desktop (1024px+):** Single column for body text (65ch max). Comparison table and approach tabs may extend slightly wider. Getting started steps use numbered list with generous vertical spacing.

### Animation / Motion

- **Tabs (integration approaches):** shadcn Tabs built-in animation. No additional Motion needed.
- **Accordion (getting started):** Radix built-in expand/collapse animation, 300-500ms, ease-out.
- **Copy button feedback:** CSS transition only (icon swap, 200ms).
- **All animations respect `prefers-reduced-motion`.**
- **No scroll-triggered animations.** This is content-heavy; animation should not distract from the information.

## Copyable Content

### Prompt: Generate a Playwright Test from Description
```
I want to create a Playwright test for the following user journey on our LMS application (ASP.NET/C#, running locally at http://localhost:5000):

User journey: "Admin creates a new training module"
1. Navigate to the login page
2. Log in with admin credentials (username: testadmin, password: use environment variable TEST_ADMIN_PASSWORD)
3. Navigate to the Training Management section
4. Click "Create New Module"
5. Fill in the module details: Title = "Fire Safety Refresher 2026", Category = "Mandatory", Duration = "30 minutes"
6. Save the module
7. Verify the module appears in the module list with status "Draft"

Generate a Playwright test script in TypeScript. Use robust selectors (prefer text content, ARIA roles, and data-testid attributes over CSS classes). Include appropriate waits and assertions at each step.

Important: if you are unsure about specific selectors or page structure, add a comment saying "VERIFY: [what to check]" rather than guessing.
```

### Prompt: Natural-Language Test Scenario Template
```
Write a plain-English test scenario description for the following user flow. This description will be used to generate and regenerate automated tests, so it needs to be:
- Specific enough that a developer (or AI) can unambiguously implement it
- Written in terms of user actions and expected outcomes, not technical selectors
- Including any preconditions or test data requirements

User flow: [describe the flow]

Format the output as:
## [Test Name]
**Preconditions:** [what must be true before the test starts]
**Steps:**
1. [action] → [expected result]
2. [action] → [expected result]
...
**Postconditions:** [what to clean up after the test]
```

### Prompt: CoWork Exploratory Test
```
I want you to test the following user journey on our staging site. Navigate through each step and report what you observe, including any errors, unexpected behaviour, or usability issues.

Site: [staging URL]
Journey: New user registration and first login

Steps to test:
1. Go to the registration page
2. Complete the registration form with test data
3. Check for confirmation email (or confirmation screen)
4. Log in with the new credentials
5. Verify the user dashboard loads correctly
6. Check that the user profile shows the correct details

For each step, report:
- What happened (screenshot description)
- Whether it matched expected behaviour
- Any issues, warnings, or unexpected states

Do not fix anything. Just observe and report.
```

### Prompt: Migrate a Ghost Inspector Test
```
I have an existing Ghost Inspector test that performs the following steps. I want to recreate this as a Playwright test.

Ghost Inspector test name: "[test name]"
What it tests: [describe the user journey in plain English]
Current selectors used: [list any known CSS selectors from Ghost Inspector, or say "unknown"]
Authentication: [how the test handles login — cookie injection, direct login, etc.]

Generate an equivalent Playwright test in TypeScript. Where the Ghost Inspector test relies on fragile CSS selectors, use more robust alternatives (text content, ARIA roles, data-testid). Add comments explaining any significant differences from the Ghost Inspector version.
```

### Example: Natural-Language Test Catalogue Entry
```
## LMS Admin: Create Training Module

**Preconditions:**
- Admin user exists with valid credentials
- Training Management module is enabled
- No module named "Test Module [timestamp]" exists

**Steps:**
1. Log in as admin → dashboard loads, "Training Management" link is visible
2. Navigate to Training Management → module list page loads
3. Click "Create New Module" → creation form appears
4. Enter title: "Test Module [timestamp]", category: "Mandatory", duration: "30 minutes" → all fields accept input
5. Click "Save" → success message appears, redirected to module list
6. Verify new module appears in list with status "Draft" → module is visible with correct details

**Postconditions:**
- Delete the test module via admin panel or API to avoid test data accumulation
```

## Two-Track Considerations

This section is **Developer track only**. The concepts (browser automation, CI/CD, Playwright, test scripts) are inherently technical and would overwhelm General track users.

However, there is one cross-reference worth noting: if a General user is interested in CoWork's browser automation for non-testing purposes (e.g., automated form filling, data entry, web research), that content belongs in section 1.7 (Recurring & Scheduled Tasks) rather than here.

No cross-track badges or General-user adaptations are needed for this section.

## Acceptance Criteria

1. The section opens with a balanced introduction that acknowledges Ghost Inspector's current role and positions AI testing as complementary, not as an immediate replacement.
2. Three AI testing approaches are presented (CoWork, Playwright MCP, Computer Use API), each with current capabilities, limitations, and a maturity badge.
3. The Ghost Inspector comparison table is present with at least 10 comparison dimensions, honestly noting where Ghost Inspector is currently stronger.
4. The "self-healing tests" advantage is clearly highlighted as the key differentiator for AI-driven testing.
5. Two integration approaches are presented (Conservative / Progressive) with clear guidance on when to use each.
6. The "Practical Starting Points" section provides a numbered, prioritised list of concrete first steps for Phew!.
7. The "Honest Limitations" callout is visually distinct (Alert component, warning variant) and covers at minimum: non-determinism, hallucinated selectors, scheduling gaps, reporting gaps, cost unpredictability.
8. At least four copyable prompts are provided: test generation, test scenario template, CoWork exploratory test, and Ghost Inspector migration.
9. Each copyable prompt uses Phew!-relevant context (LMS, ASP.NET/C#, safeguarding training modules).
10. All prompts have functional copy-to-clipboard buttons.
11. Prompts are syntax-highlighted using Shiki (language: `text` or `markdown`).
12. The comparison table is responsive: full table on desktop, scrollable or stacked on mobile.
13. All body text is at least 16px (1rem) with a maximum line length of 65ch.
14. The section is fully responsive and readable on mobile.
15. Keyboard navigation works throughout.
16. All animations respect `prefers-reduced-motion`.
17. UK English is used throughout all copy.
18. Semantic HTML: proper heading hierarchy (h2 for section title, h3 for subsections).
19. No "AI slop" design patterns.
20. Content is defined as typed TypeScript objects — not hard-coded JSX.
21. Cross-reference to section 1.11 (hallucinated selectors) is present where relevant.
22. Cross-reference to section 1.13 (Safe MCP Usage) is present for Playwright MCP installation.
23. The section does not promise capabilities that do not currently exist. All future-looking statements are clearly marked as such.

## Build Agent Checklist

### Frontend Quality Checklist

**Typography**
- [ ] Body text >= 16px (1rem), using rem units
- [ ] Max line length: 65ch for body text
- [ ] Fluid type (clamp) on headings; fixed sizes on UI controls
- [ ] Font stacks include size-adjusted fallback
- [ ] No generic fonts (Inter, Roboto, etc.) unless explicitly specified

**Colour & Theming**
- [ ] All custom colours defined in OKLCH via CSS variables
- [ ] Neutrals tinted towards brand hue (not pure grey)
- [ ] No pure black (#000) or pure white (#fff)
- [ ] Dark mode tested and functional (if applicable to section)
- [ ] All text meets WCAG AA contrast (4.5:1 body, 3:1 large/UI)

**Layout & Spacing**
- [ ] Spacing values from the 4pt grid (4, 8, 12, 16, 24, 32, 48, 64, 96)
- [ ] Visual hierarchy uses 2+ dimensions (size, weight, colour, space)
- [ ] No nested cards
- [ ] No identical repeating card grids

**Motion & Animation**
- [ ] Only `transform` and `opacity` animated (no width/height/top/left)
- [ ] Timing follows 100/300/500 rule
- [ ] Easing uses exponential curves (not default `ease` or bounce)
- [ ] `prefers-reduced-motion` handled (crossfade fallback or disable)
- [ ] Exit animations faster than entrances
- [ ] Motion used via Tailwind transitions where CSS suffices; Motion library only for layout/enter/exit

**Interaction**
- [ ] All 8 interactive states designed (default, hover, focus, active, disabled, loading, error, success)
- [ ] `:focus-visible` ring on all interactive elements (2-3px, offset, 3:1 contrast)
- [ ] Touch targets >= 44px
- [ ] Copy-to-clipboard on every prompt/template/code block
- [ ] Skeleton screens for loading states (not generic spinners)

**Accessibility**
- [ ] Semantic HTML (headings, landmarks, labels)
- [ ] Skip link present
- [ ] Keyboard navigation tested (Tab, Enter, Escape, Arrows)
- [ ] `aria-expanded`, `aria-controls` on accordions/collapsibles
- [ ] `aria-label` on icon-only buttons
- [ ] Images have `alt` text (or `alt=""` for decorative)
- [ ] Never `outline: none` without `:focus-visible` replacement

**Performance**
- [ ] No barrel file imports (direct imports from source)
- [ ] Shiki lazy-loaded (not in initial bundle)
- [ ] Derived state computed during render (no useEffect for derived values)
- [ ] `content-visibility: auto` on long scrollable content
- [ ] Passive event listeners on scroll/touch handlers
- [ ] Static JSX hoisted outside components where possible

**Responsive**
- [ ] Mobile-first (base styles for mobile, min-width queries for larger)
- [ ] Tested at 320px, 640px, 768px, 1024px widths
- [ ] No critical functionality hidden on mobile
- [ ] Hover-dependent features have touch alternatives
- [ ] Safe area insets handled for mobile

**UX Writing & Content**
- [ ] UK English throughout (spelling, grammar, currency, date format)
- [ ] Button labels use verb + object pattern
- [ ] Error messages answer: what, why, how to fix
- [ ] Consistent terminology (no synonyms for the same action)
- [ ] Empty states are actionable, not just "Nothing here"

**Code Quality**
- [ ] Tailwind utility classes only (no CSS modules or styled-components)
- [ ] `cn()` utility for conditional class merging
- [ ] `@/` path alias for all imports
- [ ] Components composed from shadcn primitives (not mega-components)
- [ ] Content defined as typed TS objects, not hard-coded JSX
